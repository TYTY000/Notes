给一个分数作为评价，是上一步的折扣* 分数 +这一步行动的奖励。
需要大量的尝试去探测奖励和惩罚，得出最佳策略。
探索时可以设置一个小概率p进行随机操作，大概率时进行理性最大化奖励的操作。
初始状态的价值奖励可以是一个启发函数，我们再定义一个行动奖励和折扣，就相当于定义了递归的初始状态和迭代过程。
可以设置目标状态或最大迭代次数/时间来作为终止条件。

![[Pasted image 20240120111725.png]]
强化学习中的 Q 值公式是基于状态转移的不确定性和策略的随机性来计算动作价值函数的期望的。这样可以使得 Q 值公式更加通用和适应性强，能够处理各种类型的环境和策略。

#莫拉维克悖论
莫拉维克悖论的意思是，人类所独有的高阶智慧能力，比如推理、创造、逻辑等，只需要非常少的计算能力，而人类的无意识的技能和直觉，比如感知、运动、语言等，却需要极大的运算能力。2

人类的感知和运动等技能已经经过了数亿年的打磨，而人类的抽象思维等技能只有几万年的历史，所以后者的实现更加低效。4


#robotics
![[Pasted image 20240120125352.png]]
