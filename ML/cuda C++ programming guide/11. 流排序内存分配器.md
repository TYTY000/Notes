动机：cudaMalloc cudaFree会导致所有的流进行同步。
分配器可以把分配和释放变成核启动和异步拷贝，并且可以控制缓存行为、进程间共享分配。
使得多个库可以共享驱动管理的内存池，避免额外的内存消耗。

###### #查询支持
注：WSL2不支持poolSupportedHandleTypes
```cpp
int main() {
  int driverVersion = 0;
  int deviceSupportsMemoryPools = 0;
  int poolSupportedHandleTypes = 0;
  int device = 0;
  cudaDriverGetVersion(&driverVersion);
    std::cout << driverVersion <<'\n';
  if (driverVersion >= 11020) {
    cudaDeviceGetAttribute(&deviceSupportsMemoryPools,
                           cudaDevAttrMemoryPoolsSupported, device);
  }
  if (deviceSupportsMemoryPools != 0) {
    std::cout << " `device` supports the Stream Ordered Memory Allocator\n"; 
  }

  if (driverVersion >= 11030) {
    cudaDeviceGetAttribute(&poolSupportedHandleTypes,
                           cudaDevAttrMemoryPoolSupportedHandleTypes, device);
  }
  if (poolSupportedHandleTypes & cudaMemHandleTypePosixFileDescriptor) {
    std::cout << " Pools on the specified device can be created with posix file descriptor-based IPC\n";
  }
}
```
释放前要调用 流/事件查询、流/事件/设备同步来避免UB。

###### #cudaMemPool_t
cudaMallocAsync用的是内存池的资源，如果没指明用的是哪个，用的就是流设备的当前内存池。
设备的内存池用cudaDevice(Set/Get)Mempool来设置/获取。如果没设置，就是默认内存池，默认池不支持IPC。
cudaDeviceGetDefaultMempool and cudaMemPoolCreate会返回池句柄。

API cudaMallocFromPoolAsync and c++ overloads of cudaMallocAsync 允许直接指定其他内存池去分配。
cudaMemPoolSetAttribute and cudaMemPoolGetAttribute可以用来访问内存池参数。
cudaMemPoolSetAccess and cudaMemPoolGetAccess来访问 访问权限。
```cpp
  int device = 0;
  cudaMemPoolProps props = {.allocType = cudaMemAllocationTypePinned,
                            .location = {
                                .type = cudaMemLocationTypeDevice,
                                .id = device,
                            }};
  CUmemoryPool pool;
  cudaMemPoolCreate(&pool, &props);
```

###### #物理页缓存行为
默认会尽量减少池拥有的物理内存，cudaMemPoolAttrReleaseThreshold
```cpp
  cuuint64_t thresh = 65535;
  cudaMemPoolSetAttribute(pool, cudaMemPoolAttrReleaseThreshold, &thresh);
```
设置为UINT64_MAX会阻止池减少内存（在调用核函数、事件和设备同步时），这样通常是为了避免隐式收缩：
```cpp
  cuuint64_t thresh = UINT64_MAX;
  cudaMemPoolSetAttribute(pool, cudaMemPoolAttrReleaseThreshold, &thresh);
  // application phase needing a lot of memory from the stream ordered allocator
  int *ptrs[10];
  int size[10];
  cudaStream_t stream;
  for (int i = 0; i < 10; i++) {
    for (int j = 0; j < 10; j++) {
      cudaMallocAsync(&ptrs[j], size[j], stream);
    }
    kernel<<<..., stream>>>(ptrs, ...);
    for (int j = 0; j < 10; j++) {
      cudaFreeAsync(ptrs[j], stream);
    }
  }

  // Process does not need as much memory for the next phase.
  // Synchronize so that the trim operation will know that the allocations are
  // no longer in use.
  cudaStreamSynchronize(stream);
  cudaMemPoolTrimTo(pool, 0);
```
同样，在进行内存池操作之前先做同步。

###### #资源使用统计
```cpp
struct usageStatistics {
  cuuint64_t reserved;
  cuuint64_t reservedHigh;
  cuuint64_t used;
  cuuint64_t usedHigh;
};

void getUsageStatistics(cudaMemPool_t memPool,
                        struct usageStatistics *statistics) {
  cudaMemPoolGetAttribute(memPool, cudaMemPoolAttrReservedMemCurrent,
                          &statistics->reserved);
  cudaMemPoolGetAttribute(memPool, cudaMemPoolAttrReservedMemHigh,
                          &statistics->reservedHigh);
  cudaMemPoolGetAttribute(memPool, cudaMemPoolAttrUsedMemCurrent,
                          &statistics->used);
  cudaMemPoolGetAttribute(memPool, cudaMemPoolAttrUsedMemHigh,
                          &statistics->usedHigh);
}

void resetStatistics(cudaMemPool_t memPool) {
  cuuint64_t value = 0;
  cudaMemPoolSetAttribute(memPool, cudaMemPoolAttrReservedMemHigh, &value);
  cudaMemPoolSetAttribute(memPool, cudaMemPoolAttrUsedMemHigh, &value);
}
```
###### #内存复用策略
在流同步前 可以复用流内部cudaFreeAsync的资源。
在CPU流同步后 其他流也可以使用。

#cudaMemPoolReuseFollowEventDependencies
在申请分配GPU内存前会先检查事件的依赖，尝试分配另一个流中释放的内存
```cpp
  int *ptr, *ptr2;
  std::size_t size = 1024;
  cudaEvent_t event;
  cudaMallocAsync(&ptr, size, stream);
  // kernel<<<..., originalStream>>>(ptr, ...);
  cudaFreeAsync(ptr, stream);
  cudaEventRecord(event, stream);

  // waiting on the event that captures the free in another stream
  // allows the allocator to reuse the memory to satisfy
  // a new allocation request in the other stream when
  // cudaMemPoolReuseFollowEventDependencies is enabled.
  cudaStreamWaitEvent(otherStream, event);
  cudaMallocAsync(&ptr2, size, otherStream);
```
#cudaMemPoolReuseAllowOpportunistic

比FollowEventDependencies更加激进，会直接尝试去找寻其他流用过释放的内存。
Disabling this policy does not stop the cudaMemPoolReuseFollowEventDependencies from applying.
```cpp
  cudaStreamWaitEvent(otherStream, event);
  cudaMallocAsync(&ptr2, size, otherStream);
  cudaMallocAsync(&ptr, size, stream);
  // kernel<<<..., originalStream>>>(ptr, ...);
  cudaFreeAsync(ptr, stream);

  // after some time, the kernel finishes running
  wait(10);

  // When cudaMemPoolReuseAllowOpportunistic is enabled this allocation request
  // can be fulfilled with the prior allocation based on the progress of
  // originalStream.
  cudaMallocAsync(&ptr2, size, otherStream);
```

#cudaMemPoolReuseAllowInternalDependencies
  最激进， 会去抢占其他流的待执行任务的内存，并且设置一个waitevent标志，在本任务完成后归还相应内存（那个流才能恢复执行）。
  
#禁用策略
  这种重用方式 不可控，每次运行结果不同，特别是抢占内存，有不确定的可能，可以禁用。
```cpp
    cudaMemPoolSetAttribute(pool, cudaMemPoolReuseAllowInternalDependencies, 0);
```

##### #设备间访问
内存池和peeraccess没有关系，用cudaMemPoolSetAccess 来确认权限。
为了能够从其他设备进行访问，访问设备必须与内存池的设备具有对等能力；用 cudaDeviceCanAccessPeer确认（**安全起见必须已有内存分配动作**）
权限是一次性的，不是按照时间段来的。
```cpp
// snippet showing usage of cudaMemPoolSetAccess:
cudaError_t setAccessOnDevice(cudaMemPool_t memPool, int residentDevice,
                              int accessingDevice) {
  cudaMemAccessDesc accessDesc = {};
  accessDesc.location.type = cudaMemLocationTypeDevice;
  accessDesc.location.id = accessingDevice;
  accessDesc.flags = cudaMemAccessFlagsProtReadWrite;

  int canAccess = 0;
  cudaError_t error =
      cudaDeviceCanAccessPeer(&canAccess, accessingDevice, residentDevice);
  if (error != cudaSuccess) {
    return error;
  } else if (canAccess == 0) {
    return cudaErrorPeerAccessUnsupported;
  }

  // Make the address accessible
  return cudaMemPoolSetAccess(memPool, &accessDesc, 1);
}
```
##### #IPC_pool
首先需要开启池的访问权限，然后共享池内特定的分享。

#共享访问 
```cpp
int shareHandle() {
  // in exporting process
  // create an exportable IPC capable pool on device 0
  cudaMemPoolProps poolProps = {};
  poolProps.allocType = cudaMemAllocationTypePinned;
  poolProps.location.id = 0;
  poolProps.location.type = cudaMemLocationTypeDevice;

  // Setting handleTypes to a non zero value will make the pool exportable (IPC
  // capable)
  poolProps.handleTypes = cudaMemHandleTypePosixFileDescriptor;

  cudaMemPoolCreate(&memPool, &poolProps);

  // FD based handles are integer types
  int fdHandle = 0;

  // Retrieve an OS native handle to the pool.
  // Note that a pointer to the handle memory is passed in here.
  cudaMemPoolExportToShareableHandle(&fdHandle, memPool,
                                     cudaMemHandleTypePosixFileDescriptor, 0);

  // The handle must be sent to the importing process with the appropriate
  // OS specific APIs.
  return fdHandle;
}
```
```cpp
void importHandle(int *fdHandle) {
  // in importing process
  // The handle needs to be retrieved from the exporting process with the
  // appropriate OS specific APIs.
  // Create an imported pool from the shareable handle.
  // Note that the handle is passed by value here.
  cudaMemPoolImportFromShareableHandle(&memPool,
                                       reinterpret_cast<void *>(fdHandle),
                                       cudaMemHandleTypePosixFileDescriptor, 0);
}
```
#在导入进程中开启访问
也需要注意要在分配/写完成后再去访问数据。
```cpp
cudaStream_t stream;
int *ptr;
std::size_t size = 1024;
struct SharedMemory {
  cudaMemPoolPtrExportData ptrData;
  cudaIpcEventHandle_t readyIpcEventHandle;
};

SharedMemory *shmem;
SharedMemory *sharePtr() {
  // preparing an allocation in the exporting process
  cudaMemPoolPtrExportData exportData;
  cudaEvent_t readyIpcEvent;
  cudaIpcEventHandle_t readyIpcEventHandle;

  // ipc event for coordinating between processes
  // cudaEventInterprocess flag makes the event an ipc event
  // cudaEventDisableTiming  is set for performance reasons

  cudaEventCreate(&readyIpcEvent,
                  cudaEventDisableTiming | cudaEventInterprocess);

  // allocate from the exporting mem pool
  cudaMallocAsync(&ptr, size, memPool, stream);

  // event for sharing when the allocation is ready.
  cudaEventRecord(readyIpcEvent, stream);
  cudaMemPoolExportPointer(&exportData, ptr);
  cudaIpcGetEventHandle(&readyIpcEventHandle, readyIpcEvent);

  // Share IPC event and pointer export data with the importing process using
  //  any mechanism. Here we copy the data into shared memory
  shmem->ptrData = exportData;
  shmem->readyIpcEventHandle = readyIpcEventHandle;
  // signal consumers data is ready
  return shmem;
}
```
```cpp
void importPtr(SharedMemory *shmem) {
  // Importing an allocation
  cudaMemPoolPtrExportData *importData = &shmem->ptrData;
  cudaEvent_t readyIpcEvent;
  cudaIpcEventHandle_t *readyIpcEventHandle = &shmem->readyIpcEventHandle;

  // Need to retrieve the ipc event handle and the export data from the
  // exporting process using any mechanism.  Here we are using shmem and just
  // need synchronization to make sure the shared memory is filled in.

  cudaIpcOpenEventHandle(&readyIpcEvent, *readyIpcEventHandle);

  // import the allocation. The operation does not block on the allocation being
  // ready.
  void **ptr;
  cudaMemPoolImportPointer(ptr, memPool, importData);

  // Wait for the prior stream operations in the allocating stream to complete
  // before using the allocation in the importing process.
  cudaStreamWaitEvent(stream, readyIpcEvent);
  kernel<<<..., stream>>>(ptr, ...);
}

```
在export的流中要用事件进行ipc绑定，在import的异步free后再在原流的池中进行异步free。
# TODO: 官方文档此处用的函数有问题，暂时没解决。


### 注意点
不能在导入的池 中进行分配，也不能进行设置（cudaMallocFromPoolAsync 也无效），内存重用的政策也是无效的。
暂时不支持IPC池直接物理释放内存。
IPC池的资源使用统计属性查询仅反映导入到进程中的分配以及关联的物理内存。
CUDA内存池作为驱动的一部分，其优化之一就是与同步API的集成。当用户请求CUDA驱动进行同步（设备的驱动，设备的同步）时，驱动会等待异步工作完成。在返回之前，驱动会确定同步保证完成的释放。这些分配可用于分配，无论指定的流或禁用的分配策略如何。驱动还会在这里检查cudaMemPoolAttrReleaseThreshold，并释放它可以释放的任何多余的物理内存。
##### 不太理解
在当前的CUDA驱动中，任何涉及到从cudaMallocAsync分配的内存的异步内存复制（cudaMemcpyAsync）都应该在调用线程的当前上下文中使用指定流的上下文。但是，对于cudaMemcpyPeerAsync，这是不必要的，因为API中指定的设备主上下文会被引用，而不是当前上下文。