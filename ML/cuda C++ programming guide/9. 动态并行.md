CUDA的动态并行提供了一种强大的机制，使得GPU能够做出简单的判断并创建调用自己的核函数，从而提高了并行计算的灵活性和效率。
编译参数-rdc=true(必需) -lcudadevrt(可省略)
#### #cuda执行环境
有点类似CPU的并行编程，但是它不会detach，默认必须子线程块要先join进行同步（子块没完成父块不会完成）。
创建的流和时间只能在当前grid（device）范围内使用，否则UB。
NULL流是默认块中的所有线程共享的，不保证执行顺序。命名流是grid中所有线程共享的，流中的执行是顺序的。
不保证在条件满足时子块就开始并行，但是一定在父块结束前进行运行。
设备运行时不支持多GPU。

#### #cuda内存模型
在device中弃用cudaDeviceSynchronize()
核函数的调用都是异步的。
进行子块的调用有点类似fork，fork之后的行为和逻辑不可控制和获取。
**如果想要获取子块的结果，应该在子块执行参数的流表明cudaStreamTailLaunch$^{1}$。
cudaStreamTailLaunch实际上是一个flag，在该流中启动的内核将异步执行，并在完成时恢复运行。**
执行完要在host中可见，运行cudaDeviceSynchronize()。
#### 零拷贝内存是一种特殊类型的内存，它允许CPU和GPU共享同一块物理内存，从而避免数据拷贝。你可以通过指针访问这些内存，就像访问主机内存一样。
#### _常量内存不能在device中修改，必须在子块创建前完成。_
#### _共享和局部内存是私有的，不能在父块子块间共享。可以用\_\_isGlobal()来判断传递是否安全。
cudaMemcpy*Async() or cudaMemset*Async()的调用也不能使用共享和局部内存。
### 所有传递给线程执行的参数都要有全局\_\_device\_\_标记、或被new()、cudaMalloc()分配。

#### 纹理内存同$^{1}$


#动态并行语法
1.调用 同普通。
2.启动参数， 会继承父块的参数，比如说设备参数、限制及shared memory and L1 cache size 等。
3.流。名流可以被网格内的任何线程使用，但是流句柄不能传递给其他子/父内核。换句话说，流应该被视为创建它的网格私有的。
#### 不能依赖不同流对象之间的并行性，没有并行保证，会UB。

#cuda流
流应该被视为创建它的网格私有的，流句柄不能传递给其他子/父内核。
1.默认流中的工作没有依赖关系，自动异步运行。
2.命名流必须使用cudaStreamNonBlocking标志创建，……（补充？）
3.即发即忘流就是用于派发一些简单的计算，不能用Events，用法如下：
```c
// In this example, C2's launch will not wait for C1's completion
__global__ void P( ... ) {
   C1<<< ... , cudaStreamFireAndForget >>>( ... );
   C2<<< ... , cudaStreamFireAndForget >>>( ... );
}
```
4.尾部启动流，用于获取需要即时结算任务的结果，有点类似C++的exit function，不能用Events，用法如下：
```cpp
// In this example, P2 will only launch after C completes.
__global__ void P1( ... ) {
   C<<< ... , cudaStreamTailLaunch >>>( ... );
}

__global__ void P2( ... ) {
}

int main ( ... ) {
   ...
   P1<<< ... >>>( ... );
   P2<<< ... >>>( ... );
   ...
}
```
#事件
支持同步流间的时间。
This means that cudaStreamWaitEvent() is supported, but cudaEventSynchronize(), cudaEventElapsedTime(), and cudaEventQuery() are not. 
As cudaEventElapsedTime() is not supported, cudaEvents must be created via cudaEventCreateWithFlags(), passing the cudaEventDisableTiming flag.
和命名流一样，事件只能在线程块内使用。
```c
// 创建两个流
cudaStream_t stream1, stream2;
cudaStreamCreate(&stream1);
cudaStreamCreate(&stream2);

// 在stream1中做一些操作
// ...

// 创建一个事件
cudaEvent_t event;
cudaEventCreateWithFlags(&event, cudaEventDisableTiming);

// 在stream1中记录这个事件
cudaEventRecord(event, stream1);

// 在stream2中等待这个事件完成
cudaStreamWaitEvent(stream2, event, 0);

// 在stream2中做一些操作
// ...

// 销毁事件和流
cudaEventDestroy(event);
cudaStreamDestroy(stream1);
cudaStreamDestroy(stream2);
```
#同步 
和CPU编程一样，不要试图显式控制子块的进度，只能通过事件来进行同步。

#设备运行时管理
运行时不支持对其他device控制的API。
一些设备API（如cudaSetDevice()）不受支持，而cudaGetDevice()和cudaDeviceGetAttribute()则可以使用
具体的API接口略，需要注意的是：
\1.host/device上分配的指针只能host/device上释放，host的cudaMalloc是在设备全局未用内存上分配，device是在设备堆上分配，要少一些（cudaLimitMallocHeapSize）
2.cudaMemset/cpy(2D/3D)Async
	仅支持异步 memcpy/set 函数，仅允许设备内 memcpy、不能传入本地或共享内存指针。
3.cudaStreamCreateWithFlags
	Must pass cudaStreamNonBlocking flag，都是异步的。
4.cudaEventCreateWithFlags
	Must pass cudaEventDisableTiming flag，异步无法计时。
5.cudaGetLastError
	Last error is per-thread state, not per-block state
6.cudaGetDevice
	Always returns current device ID as would be seen from host
7.cudaDeviceGetAttribute
本地设备参数查询，这个函数的执行速度非常快，比cudaGetDeviceProperties函数快了四个数量级。
8.%smid and %warpid是会动态改变的，依赖 %smid 或 %warpid 在线程或线程块的生命周期中保持不变是不安全的。

#性能 
**开启了动态并行会有一定开销，事务无法达到峰值速度。**

内存占用：CUDA 设备运行时系统软件为各种管理目的预留了内存，特别是用于跟踪待处理的网格启动。配置控制选项可以减小此预留的大小，但可能会带来某些启动限制。这样做的目的是为了在内存有限的情况下，通过调整配置来平衡内存使用和性能。

待处理的内核启动：当内核启动时，所有相关的配置和参数数据都会被跟踪，直到内核完成。这些数据存储在系统管理的启动池中。通过调用 cudaDeviceSetLimit() 并指定 cudaLimitDevRuntimePendingLaunchCount，可以配置固定大小启动池的大小。这样做可以控制并发内核的数量，以防止过多的内核启动导致资源耗尽。

配置选项：通过主机程序的 cudaDeviceSetLimit() API 控制设备运行时系统软件的资源分配。在启动任何内核之前，必须设置限制，并且在 GPU 积极运行程序时，不能更改这些限制。这样做是为了确保一旦 GPU 开始运行程序，其资源分配将保持稳定，不会因为更改配置而导致运行时错误。

内核不会直接报错，必须回到host端才能进行错误处理。
至于是抛异常还是继续执行依赖于错误和配置。

#cudaLimitDevRuntimePendingLaunchCount
控制为缓冲内核启动和由于未解决的依赖关系或缺乏执行资源而尚未开始执行的事件预留的内存量。当缓冲区已满时，在设备端内核启动期间尝试分配启动槽将失败并返回 cudaErrorLaunchOutOfResources，而尝试分配事件槽将失败并返回 cudaErrorMemoryAllocation。启动槽的默认数量为 2048。应用程序可以通过设置 cudaLimitDevRuntimePendingLaunchCount 来增加启动和/或事件槽的数量。分配的事件槽数量是该限制值的两倍。

#cudaLimitStackSize
控制每个 GPU 线程的堆栈大小（以字节为单位）。 CUDA 驱动程序会根据需要自动增加每个内核启动的每线程堆栈大小。每次启动后，此大小不会重置回原始值。要将每个线程堆栈大小设置为不同的值，可以调用 cudaDeviceSetLimit() 来设置此限制。堆栈将立即调整大小，如有必要，设备将阻塞，直到所有先前请求的任务完成。可以调用 cudaDeviceGetLimit() 来获取当前每线程堆栈大小。

#CDP2
Cuda 12.0+ or compute_9.0+
只支持64位
引入了尾部启动 (cudaStreamTailLaunch) 和即发即弃 (cudaStreamFireAndForget) 命名流。
不再为不适合固定大小池的待启动启动提供虚拟化池。 
同时存在的事件总数有限制（仅在启动完成后才会被销毁），等于待启动计数的两倍。 cudaLimitDevRuntimePendingLaunchCount 必须设置得足够大，以避免耗尽启动槽和事件​​槽。