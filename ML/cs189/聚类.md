1.层次聚类  根据数据相似性分类
2.划分聚类    
3.模型聚类  

- 类内高密度
- 类间低密度
- 不能按照相似度/距离来判断

#K-means 
k均值算法
- 随机选择簇中心
- 将点分配给最近的簇中心，不断计算新的簇中心
- 直到收敛或者达到最大迭代为止
![[Pasted image 20240115164612.png]]

![[Pasted image 20240116103135.png]]
![[Pasted image 20240116104645.png]]
#MoG 
![[Pasted image 20240116113127.png]]
![[Pasted image 20240116114657.png]]![[Pasted image 20240116114643.png]]![[Pasted image 20240116115127.png]]

#多向分类 
加权0-1损失
![[Pasted image 20240116123701.png]]


![[Pasted image 20240116150132.png]]
![[Pasted image 20240116151943.png]]
ROC：receiver operating characteristics
越接近左上越好，如何用一个指标去衡量？积分。Area Under Curve

![[Pasted image 20240116154026.png]]
PRC越接近右上越好，积分。Average precision

#KNN 
用预测点的特征去逼近数据集中最近的特征值，求平均值并输出是KNN回归，求最多次数是KNN分类。
k一般是根据模型的正确率来选择，通过循环迭代计算，我们能得到最低的错误率，选择其对应的k值。
k越大，越容易欠拟合，k越小，越容易过拟合。
![[Pasted image 20240117125633.png]]
![[Pasted image 20240117125744.png]]
1.数据量足够大时，1NN 的误差不会超过两倍的贝叶斯误差
2.无穷NN的误差->贝叶斯误差
即：1NN的误差最多不超过无穷NN的误差的两倍。
优点：
速度快
快速学习复杂函数
缺点：
拟合高维数据很差
测试时间很长
需要很大的存储空间
☆  还需要一个距离衡量方法！mahanttan，euclidian....
![[Pasted image 20240117150446.png]]

#对比学习
对比学习的基本思想是，给定一个数据点，通过一些数据增强或变换的方法，生成不同的视图（view），然后用一个编码器（encoder）将这些视图转换为向量表示（representation）。接着，通过计算这些向量之间的相似度或距离，来最大化同一个数据点的不同视图之间的相似度，同时最小化不同数据点的不同视图之间的相似度。这样，模型就能够学习到数据的内在结构和语义信息，从而提高模型的泛化能力和迁移能力。