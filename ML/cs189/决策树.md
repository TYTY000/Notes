决策树就是把一部分状态空间放进了搜索树中，是一个贪心算法，会把优先级高的特征放在靠近树的根部位置
也有点像对比学习，用后验经验来分类，而不是用一个具有超参数的先验模型去拟合分布。
决策树需要一个大数据集去防止出现欠拟合的问题
如何选择feature？选择信息熵小的（分类更纯）
![[Pasted image 20240118131614.png]]
![[Pasted image 20240118131731.png]]
![[Pasted image 20240118132344.png]]
停止条件：
1.信息熵为0或足够小（类别一致，再分也没有意义）
2.同输出特征
3.达到最大深度/最小信息增益

前剪枝：
交叉验证
最小描述长度

后剪枝：
统计验证
保留集性能
![[Pasted image 20240118134729.png]]
![[Pasted image 20240118134914.png]]
![[Pasted image 20240118135301.png]]

#random_forest 集成学习
对每个决策树：
有放回的抽取一定量数据、随机特征范围中选择最重要的特征。
**期望不变，可以减少相关性，减少最终数据的方差。**
![[Pasted image 20240118141232.png]]
#boosting
对之前预测错误的点赋予更高的权值。
#AdaBoost
```
均衡权重值w=1/n
for i in range(M):
	classifier G(w)
	em = sum(w of errors)/sum(w)
	![[Pasted image 20240125154822.png]]
	(optional) normalize w
```
![[Pasted image 20240125154850.png]]
